{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narendra974/AIMLOPS_IISC/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Kafka and SPARK"
      ],
      "metadata": {
        "id": "0gTv-7nwLC8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1 (7 marks)\n",
        "\n",
        "1. Setup Apache Kafka and Apache Spark in your local system.\n",
        "2. Install and start Kafka and Zookeper.\n",
        "3. Create a kafka topic named \"assignment-4\".\n",
        "4. Start the Kafka producer and write few events into the topic.\n",
        "5. Consume the events using Spark structured streaming.\n",
        "6. Document all the steps and the commands used in markdown format including all your codes.\n",
        "\n",
        "## Use the below format\n",
        "\n",
        "- Steps/command description\n",
        "```\n",
        "<command>\n",
        "```\n",
        "\n",
        "- *File: <FileName.py>*\n",
        "``` file\n",
        "codes...\n",
        "```"
      ],
      "metadata": {
        "id": "1u3IftTwekW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After successful installation of KAFKA, SPARK, JAVA and PYTHON\n",
        "# Stat the zookeeper with the below command\n",
        "\n",
        "### .\\bin\\windows\\zookeeper-server-start.bat .\\config\\zookeeper.properties\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4bYN1UZxCcxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the Kafka Server\n",
        "\n",
        "###  .\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties"
      ],
      "metadata": {
        "id": "GDvh-FFPCwKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create 'assighnment-4' topic\n",
        "\n",
        "### .\\bin\\windows\\kafka-topics.bat --create --bootstrap-server localhost:9092 --replication-factor 1  --topic \"assighnment-4\""
      ],
      "metadata": {
        "id": "i_gG3j_3EPsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the producer.\n",
        "\n",
        "### .\\bin\\windows\\kafka-console-producer.bat --broker-list localhost:9092 --topic \"assighment-4\"\n",
        "\n",
        "# Send some JSON data like below\n",
        "\n",
        "### {\"Name: \"John\", \"Age\":\"31\", \"Gender\":\"Male\"}"
      ],
      "metadata": {
        "id": "d4Nve3_pEioE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from pyspark shell load the spark steam\n",
        "\n",
        "   df = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "  .option(\"subscribe\", \"assighnment4\") \\\n",
        "  .option(\"startingOffsets\", \"latest\") \\\n",
        "  .load()"
      ],
      "metadata": {
        "id": "oP1t2mEgGp_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## all the data in kafka topic is stored as key value along with timestamp.\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "base_df = sampleDataframe.selectExpr(\"CAST(value as STRING)\", \"timestamp\")"
      ],
      "metadata": {
        "id": "aHIWsaBqG8gC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define the JSON Schema that is sent from the consumer.\n",
        "\n",
        "sample_schema = (\n",
        "        StructType()\n",
        "        .add(\"Name\", StringType())\n",
        "        .add(\"Age\", IntType())\n",
        "        .add(\"Gender\", StringType())\n",
        "    )\n",
        "info_dataframe = base_df.select(\n",
        "        from_json(col(\"value\"), sample_schema).alias(\"sample\"), \"timestamp\"\n",
        "    )"
      ],
      "metadata": {
        "id": "PfJiixgrHW1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the data with the below code.\n",
        "\n",
        "info_df_fin = info_dataframe.select(\"sample.*\", \"timestamp\")"
      ],
      "metadata": {
        "id": "jHVPRgoUHu01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2 (3 Marks)\n",
        "\n",
        "\n",
        "What is the expected output of the following PySpark code? What would be the values stored inside `marksRDD`, `courseRDD` and `pairRDD`? *(theoretical question, you are free to implement it though)*\n",
        "\n",
        "```\n",
        "marksRDD = sc.textFile(“marks.csv”);\n",
        "courseRDD = marksRDD.filter(lambda x : x.split(“,”)[1] == “DA204”);\n",
        "pairRDD = courseRDD.map(lambda y :  \n",
        "( y.split(“,”)[0], float(y.split(“,”)[2]) ) );\n",
        "gradesRDD = pairRDD.map(lambda k,v:  \n",
        "  (k, “A”) if v > 80 else  \n",
        "    ((k, “B”) if v > 70 else  \n",
        "       ((k, “C”) if v > 60 else (k, “D”))\n",
        "    )\n",
        "  );\n",
        "\n",
        "gradesRDD.collect();\n",
        "```\n",
        "\n",
        "**Marks.csv**\n",
        "```\n",
        "1,DA204,90\n",
        "2,DS211,85\n",
        "3,DA204,50\n",
        "4,DS211,40\n",
        "5,DA204,70\n",
        "6,DS220,72\n",
        "7,DA204,65\n",
        "8,DS220,69\n",
        "9,DA204,75\n",
        "```"
      ],
      "metadata": {
        "id": "YXgI8bNNJcf1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chpMulvtHV8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some useful references\n",
        "### Kafka\n",
        "- https://kafka.apache.org/quickstart\n",
        "- https://kafka.apache.org/documentation/#intro_concepts_and_terms\n",
        "\n",
        "### Spark\n",
        "- https://spark.apache.org/\n",
        "- https://medium.com/linkit-intecs/pyspark-with-google-colab-d964fd693ca7\n",
        "\n",
        "### Kafka with spark\n",
        "- https://adinasarapu.github.io/posts/2020/01/blog-post-kafka/\n",
        "- https://adinasarapu.github.io/posts/2021/02/blog-post-kafka-spark-streaming/\n",
        "\n",
        "### Kafka with tensorflow\n",
        "- https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb#scrollTo=f6qxCdypE1DD\n",
        "\n",
        "### Example(using docker):\n",
        "- https://github.com/cordon-thiago/spark-kafka-consumer\n"
      ],
      "metadata": {
        "id": "8zVsI5fHliv_"
      }
    }
  ]
}