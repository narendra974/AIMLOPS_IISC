{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO0bZr9JZLJklwsdgFffyut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narendra974/AIMLOPS_IISC/blob/main/NMT_TRANSFORMERS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcBLBDrOw_35",
        "outputId": "7c1b54df-906e-48a6-cd1a-7380d27d82b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DATASETS/ENG_DEU_DATA/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH5dpmIYxXjQ",
        "outputId": "229033ef-fcad-4933-e792-080834a6537b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DATASETS/ENG_DEU_DATA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycld2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8F6nrVRxqNk",
        "outputId": "b254930c-0893-4da0-acb6-6545981e3699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycld2 in /usr/local/lib/python3.10/dist-packages (0.41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pycld2 as cld2\n",
        "import spacy\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import regex as re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "EQfg9rSIx4rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "!python -m spacy download de_core_news_sm\n",
        "gernlp = spacy.load('de_core_news_sm')\n",
        "engnlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "cdECDNilyFMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removestop(text,stopwords):\n",
        "  raw = text.split()\n",
        "  words = [word for word in raw if not word in stopwords]\n",
        "  cleanwords = ' '.join(words)\n",
        "  return cleanwords\n",
        "\n",
        "def tolower(text):\n",
        "  return text.lower()\n",
        "\n",
        "def removespecial(text):\n",
        "  te1 = re.sub(\"\\s+\",\" \",text)\n",
        "  te2 = re.sub('\\n', '', te1)\n",
        "  te3 = re.sub('\\r', '', te2)\n",
        "  te4 = re.sub(\"[0-9]\",\"\",te3)\n",
        "  te5 = re.sub(\"()@%^&*-_,/\\{}[?|$|.|!]\",\"\",te4)\n",
        "  te6 = re.sub(r\"[\\p{Cc}\\p{Cs}]+\",\"\",te5)\n",
        "  te7 = re.sub(r'[^\\w\\s]','', te6)\n",
        "  te8 = re.sub(\"[^a-zA-Z ]\",\"\",te7)\n",
        "  return te7\n",
        "\n",
        "def removeurl(text):\n",
        "  return re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    pattern = re.compile(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\")\n",
        "    text = re.sub(pattern,' ',text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "oIVXM3EIyp2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basefolder =  \"/content/drive/MyDrive/DATASETS/ENG_DEU_DATA/\"\n",
        "# germanfiles = [\"commoncrawl_de_en.txt\",\"europarl-v7_de_en.txt\",\"news-commentary-v9_de_en.txt\"]\n",
        "# engfiles = [\"commoncrawl_en_de.txt\",\"europarl-v7_en_de.txt\",\"news-commentary-v9_en_de.txt\"]\n",
        "germanfiles = [\"news-commentary-v9_de_en.txt\"]\n",
        "engfiles = [\"news-commentary-v9_en_de.txt\"]"
      ],
      "metadata": {
        "id": "l0kOP-4ZyV4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_files(fileloc, language):\n",
        "  with open(fileloc,\"rb\") as f:\n",
        "    f_lines = f.readlines()\n",
        "  df = pd.DataFrame(f_lines)\n",
        "  dfc = df.set_axis([language],axis=1)\n",
        "  dfc[language] = dfc[language].str.decode(\"utf-8\")\n",
        "  return dfc"
      ],
      "metadata": {
        "id": "k5IkrZZHyvf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappend=pd.DataFrame()\n",
        "for efile in range(len(germanfiles)):\n",
        "  germanfilepath = basefolder+germanfiles[efile]\n",
        "  print(germanfilepath)\n",
        "  germandff = read_files(germanfilepath,\"german\")\n",
        "  engfilepath = basefolder+engfiles[efile]\n",
        "  engdff = read_files(engfilepath,\"english\")\n",
        "  print(germandff.shape)\n",
        "  print(engdff.shape)\n",
        "  dfconcat = pd.concat([germandff, engdff],axis=\"columns\")\n",
        "  dfappend=pd.concat([dfappend, dfconcat])"
      ],
      "metadata": {
        "id": "d1jGuFJMyzR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappend.isna().sum()"
      ],
      "metadata": {
        "id": "OBH2yb4QzZGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappend.duplicated().sum()"
      ],
      "metadata": {
        "id": "9nxiAyjkzcR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappend.drop_duplicates(subset=None, keep='first', inplace=True)\n",
        "dfappend.shape"
      ],
      "metadata": {
        "id": "7l9j3y8mzkgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "german_stop_words = stopwords.words('german')\n",
        "english_stop_words = stopwords.words('english')\n",
        "\n",
        "dfappend['english_clean'] = dfappend['english'].apply(lambda x: removestop(x,english_stop_words))\n",
        "dfappend['german_clean'] = dfappend['german'].apply(lambda x: removestop(x,german_stop_words))\n",
        "dfappend['german_clean'] = dfappend['german_clean'].apply(lambda x: removespecial(x))\n",
        "dfappend['english_clean'] = dfappend['english_clean'].apply(lambda x: removespecial(x))"
      ],
      "metadata": {
        "id": "GurWsMN50fwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def langdet(x):\n",
        "  isReliable, textBytesFound, details = cld2.detect(x)\n",
        "  return(details[0][1])"
      ],
      "metadata": {
        "id": "KYehwk6C7IgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappend['is_eng'] = dfappend['english_clean'].apply(lambda x: langdet(x))\n",
        "dfappend['is_ger'] = dfappend['german_clean'].apply(lambda x: langdet(x))"
      ],
      "metadata": {
        "id": "PQF2cq057NJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(dfappend[\"is_ger\"].value_counts())"
      ],
      "metadata": {
        "id": "53KC07Ov72E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(dfappend[\"is_eng\"].value_counts())"
      ],
      "metadata": {
        "id": "6i9NdCLV8Pw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappendclean = dfappend[(dfappend.is_eng == 'en') & (dfappend.is_ger =='de')]"
      ],
      "metadata": {
        "id": "nJM6MikS8amA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappendclean.shape"
      ],
      "metadata": {
        "id": "PoGg225d8kea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(dfappendclean)"
      ],
      "metadata": {
        "id": "fPzF_1Qv8qSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfappendclean[\"engcount\"]=dfappendclean['english_clean'].str.split().str.len()"
      ],
      "metadata": {
        "id": "snjjFtCc9-QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=10000\n",
        "dfappendclean = dfappendclean.sample(n=14000)\n",
        "dfappendclean_val = dfappendclean[10000:]\n",
        "dfappendclean = dfappendclean[0:100000]"
      ],
      "metadata": {
        "id": "jV5eOKElDTdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bos_string = 'bos '\n",
        "eos_string = ' eos'\n",
        "dfappendclean['english_clean'] = bos_string + dfappendclean['english_clean'].astype(str) + eos_string\n",
        "dfappendclean['german_clean'] = bos_string + dfappendclean['german_clean'].astype(str) + eos_string\n",
        "\n",
        "en_list =  dfappendclean['english_clean'].astype(str).tolist()\n",
        "ge_list =  dfappendclean['german_clean'].astype(str).tolist()\n",
        "en_list_val =  dfappendclean_val['english_clean'].astype(str).tolist()\n",
        "ge_list_val =  dfappendclean_val['german_clean'].astype(str).tolist()\n"
      ],
      "metadata": {
        "id": "h1DnFiSKDbVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(en_list[0:5])"
      ],
      "metadata": {
        "id": "9TfAz0cIEGMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(ge_list[0:5])"
      ],
      "metadata": {
        "id": "YSw7rMxREM7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS=128\n",
        "en_tokenizer = Tokenizer();\n",
        "en_tokenizer.fit_on_texts(dfappendclean[\"english_clean\"])\n",
        "\n",
        "en_bos_index = en_tokenizer.word_index['bos']\n",
        "print(en_bos_index)\n",
        "en_eos_index = en_tokenizer.word_index['eos']\n",
        "print(en_eos_index)"
      ],
      "metadata": {
        "id": "KPhqYJzf_7_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ge_tokenizer = Tokenizer();\n",
        "ge_tokenizer.fit_on_texts(dfappendclean[\"german_clean\"])\n",
        "\n",
        "ge_bos_index = ge_tokenizer.word_index['bos']\n",
        "print(ge_bos_index)\n",
        "ge_eos_index = ge_tokenizer.word_index['eos']\n",
        "print(ge_eos_index)"
      ],
      "metadata": {
        "id": "Kg31udoEJc5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS=128\n",
        "en = en_tokenizer.texts_to_sequences(en_list)     # Output is ragged.\n",
        "en = tf.keras.utils.pad_sequences(en, maxlen=MAX_TOKENS, padding='post')\n",
        "\n",
        "ge = ge_tokenizer.texts_to_sequences(ge_list)\n",
        "ge = tf.keras.utils.pad_sequences(ge, maxlen=MAX_TOKENS+1, padding='post')\n",
        "\n",
        "en_val = en_tokenizer.texts_to_sequences(en_list_val)     # Output is ragged.\n",
        "en_val = tf.keras.utils.pad_sequences(en_val, maxlen=MAX_TOKENS, padding='post')\n",
        "\n",
        "ge_val = ge_tokenizer.texts_to_sequences(ge_list_val)\n",
        "ge_val = tf.keras.utils.pad_sequences(ge_val, maxlen=MAX_TOKENS+1, padding='post')\n"
      ],
      "metadata": {
        "id": "q7WmtCIFGtMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((en, ge))\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((en_val, ge_val))"
      ],
      "metadata": {
        "id": "Gl2vCnjq21ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "def prepare_batch(en, ge):\n",
        "\n",
        "    ge_inputs = ge[:, :-1]  # Drop the [END] tokens\n",
        "    ge_labels = ge[:, 1:]   # Drop the [START] tokens\n",
        "\n",
        "    return (en, ge_inputs), ge_labels\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "dataset_batches = make_batches(dataset)\n",
        "dataset_batches_val = make_batches(dataset_val)"
      ],
      "metadata": {
        "id": "kPCJcclNXNnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (pt, en), en_labels in dataset_batches.take(1):\n",
        "  break\n",
        "\n",
        "print(pt.shape)\n",
        "print(en.shape)\n",
        "print(en_labels.shape)"
      ],
      "metadata": {
        "id": "i49kSPXd5tQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "zl-TIYFgr9V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Zb5EHRCIsQH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "qdNlxpTmsX9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "UlwRgvGH9Rws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "laK9VsbR9bhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ABwjtYoE9fUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "YcK04HCb9jFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "2AugLcw09kna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "Bd7fjkrE9qSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "YUxJrAa29vZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ],
      "metadata": {
        "id": "wjC6tbE09y-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Io4e9uNd93mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "7TXwHV0M97YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_tokenizer.word_index)+1;\n",
        "ge_vocab_size = len(ge_tokenizer.word_index)+1;\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size = en_vocab_size,\n",
        "    target_vocab_size = ge_vocab_size,\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "7dx2CYlLDGSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "fcaLfs1qPxBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "ANc7y6QmP18L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "jFZama36QOd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ],
      "metadata": {
        "id": "1ruovAGOQSwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"NMT_ENG_GER-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "U2_jt308Rz48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(dataset_batches, epochs=1, validation_data=dataset_batches_val)"
      ],
      "metadata": {
        "id": "Cp4g1AOxQUHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, en_tokenizer, ge_tokenizer, transformer):\n",
        "    self.en_tokenizer = en_tokenizer\n",
        "    self.ge_tokenizer = ge_tokenizer\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "\n",
        "    # assert isinstance(sentence, tf.Tensor)\n",
        "    # if len(sentence.shape) == 0:\n",
        "    #  sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.en_tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    start_end = self.en_tokenizer.texts_to_sequences([''])\n",
        "    print(start_end)\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    text = ge_tokenizer.texts_to_sequences(output)[0]  # Shape: `()`.\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "    return text, attention_weights"
      ],
      "metadata": {
        "id": "90ryPhUFHzs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(en_tokenizer, ge_tokenizer, transformer)"
      ],
      "metadata": {
        "id": "W669EF07SlcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "metadata": {
        "id": "fPJsCF5LUePf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"As result creation saints becoming important way retaining faithful\"\n",
        "ground_truth = 'Infolgedessen entwickelt Ernennung neuer Heiliger zunehmend wichtigen Methode Bindung Gl√§ubigen'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "metadata": {
        "id": "68LBNC3qJSEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}